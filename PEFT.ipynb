{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Parameter-Efficient Fine-Tuning**\n",
        "\n",
        " (PEFT) is a way to fine-tune big AI models by changing only a small part of them. Instead of updating the entire model, you adjust or add tiny parts (like small layers or extra input tweaks) to make the model work well for specific tasks. This saves time, computer power, and memory while still getting great results.\n"
      ],
      "metadata": {
        "id": "hiAtR5Gq5iNn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Key Concepts of PEFT**\n",
        "\n",
        "**Efficiency:**\n",
        "      \n",
        "Only a small fraction of the model's parameters are updated during fine-tuning, reducing the computational cost and memory requirements.\n",
        "        \n",
        "The pre-trained model remains mostly intact, making the process faster and more resource-efficient.\n",
        "\n",
        "**Adaptability:**\n",
        "        \n",
        "PEFT allows a model to be adapted to multiple tasks by fine-tuning only a small task-specific parameter set while keeping the core model shared across tasks.\n",
        "\n",
        "# **Techniques:**\n",
        "        \n",
        "**Adapters:** Insert small neural networks (adapter layers) into the pre-trained model, fine-tuning only these new layers.\n",
        "            \n",
        "**LoRA (Low-Rank Adaptation):** Adjust only low-rank components of the weight matrices to capture task-specific nuances.\n",
        "            \n",
        "**Prefix Tuning:** Add trainable vectors (prefixes) to the input sequences, keeping the main model unchanged.\n",
        "            \n",
        "**Prompt Tuning:** Optimize task-specific prompts while freezing the model parameters."
      ],
      "metadata": {
        "id": "sY-So_ue55Rm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Quantization**\n",
        "\n",
        "It is essentially converting higher-memory formats (like 32-bit floating-point numbers) into lower-memory formats (like 8-bit integers) to save space and make computations faster and more efficient."
      ],
      "metadata": {
        "id": "ae5xUoLE67KA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why Quantization is Used:**\n",
        "\n",
        "**Smaller Model Size:** Lower precision means less memory is required to store the model.\n",
        "    \n",
        "**Faster Computation:** Operations with lower-precision numbers are faster and more efficient, especially on specialized hardware like GPUs or TPUs.\n",
        "    \n",
        "**Energy Efficiency:** Reduces the energy needed for computations, which is useful for running models on edge devices like smartphones.\n",
        "\n",
        "\n",
        "\n",
        "# **Types of Quantization:**\n",
        "\n",
        "\n",
        "\n",
        "**Post-Training Quantization:** Applied to a pre-trained model without retraining.\n",
        "    \n",
        "**Quantization-Aware Training:** The model is trained while considering the effects of quantization, often resulting in better accuracy."
      ],
      "metadata": {
        "id": "P_cI4itl7D3m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Note:LORA & QLORA We can loss some amount of data**"
      ],
      "metadata": {
        "id": "F9YnNfN77vJ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LoRA (Low-Rank Adaptation)**\n",
        "\n",
        "Instead of changing the whole model, LoRA adds small extra parts to the model and only trains those. This makes fine-tuning faster and uses less memory.\n",
        "\n",
        "**QLoRA (Quantized Low-Rank Adaptation)**\n",
        "\n",
        "Combines LoRA with quantization, using a smaller, 4-bit version of the model.\n",
        "    \n",
        "Further reduces memory and allows fine-tuning of large models on limited hardware.\n",
        "\n",
        "Key Difference: QLoRA applies LoRA to a quantized model for even greater efficiency."
      ],
      "metadata": {
        "id": "03mxSxGm8mVI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0eiWcxIY5q1D"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}